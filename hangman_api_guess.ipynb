{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppzV9XDrerLm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Integrates the ensemble model (CatBoost + XGBoost) with heuristic methods\n",
        "to enhance the Hangman guessing strategy. This script interacts with the\n",
        "Hangman API to play games using the improved guessing algorithm.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from typing import Optional, Dict, List\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Disable SSL warnings\n",
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ram6mg8berLw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the Ensemble Model Class\n",
        "class MultiLabelEnsemble:\n",
        "    \"\"\"\n",
        "    Loads XGBoost and CatBoost multi-label models,\n",
        "    generates probability predictions, and uses meta-classifiers\n",
        "    to combine them into final predictions.\n",
        "    \"\"\"\n",
        "    def __init__(self, catboost_model_path, xgboost_model_path, meta_classifier_path=None):\n",
        "        # Load the pre-trained CatBoost & XGBoost multi-label models\n",
        "        self.catboost_model = self._load_model(catboost_model_path)\n",
        "        self.xgboost_model = self._load_model(xgboost_model_path)\n",
        "\n",
        "        # Load meta-classifiers if provided, else initialize\n",
        "        if meta_classifier_path and os.path.exists(meta_classifier_path):\n",
        "            with open(meta_classifier_path, 'rb') as f:\n",
        "                self.meta_classifiers = pickle.load(f)\n",
        "        else:\n",
        "            self.meta_classifiers = [LogisticRegression() for _ in range(26)]\n",
        "\n",
        "    def _load_model(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        return model\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Generate probability predictions for each letter using the ensemble model.\n",
        "\n",
        "        Parameters:\n",
        "            X (pd.DataFrame or np.array): Feature matrix of shape (n_samples, 80)\n",
        "\n",
        "        Returns:\n",
        "            np.array: Probability matrix of shape (n_samples, 26)\n",
        "        \"\"\"\n",
        "        # Get CatBoost probabilities\n",
        "        catboost_pred_proba = self.catboost_model.predict_proba(X)  # Shape: (n_samples, 26)\n",
        "\n",
        "        # Get XGBoost probabilities\n",
        "        xgboost_pred_proba = self.xgboost_model.predict_proba(X)  # Shape: (n_samples, 26)\n",
        "        # Concatenate predictions\n",
        "        stacked_features = np.concatenate([catboost_pred_proba, xgboost_pred_proba], axis=1)  # Shape: (n_samples, 52)\n",
        "\n",
        "        # Predict with meta-classifiers\n",
        "        final_preds = []\n",
        "        for i in range(26):\n",
        "            probs_i = self.meta_classifiers[i].predict_proba(stacked_features)[:, 1]  # Probability of class '1'\n",
        "            final_preds.append(probs_i)\n",
        "\n",
        "        return np.array(final_preds).T  # Shape: (n_samples, 26)\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self.meta_classifiers, f)\n",
        "\n",
        "    def load_meta_classifiers(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.meta_classifiers = pickle.load(f)\n",
        "\n",
        "# Helper Functions and Mappings\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "value_map = {ch: idx+1 for idx, ch in enumerate(alphabet)}\n",
        "value_map['_'] = 0\n",
        "value_rev = {v: k for k, v in value_map.items()}\n",
        "\n",
        "def encode_word(trial_word):\n",
        "    \"\"\"\n",
        "    Encodes the current word state into an 80-length feature vector.\n",
        "\n",
        "    Parameters:\n",
        "        trial_word (str): Current word state with underscores (e.g., \"_pp_e\")\n",
        "\n",
        "    Returns:\n",
        "        np.array: Encoded feature vector of length 80\n",
        "    \"\"\"\n",
        "    word_length = len(trial_word)\n",
        "    encoded = np.full(80, -1, dtype=np.int8)\n",
        "\n",
        "    for i, ch in enumerate(trial_word):\n",
        "        if ch != '_':\n",
        "            encoded[i] = value_map.get(ch, 0)\n",
        "            encoded[80 - word_length + i] = value_map.get(ch, 0)\n",
        "        else:\n",
        "            encoded[i] = 0\n",
        "            encoded[80 - word_length + i] = 0\n",
        "    return encoded\n",
        "\n",
        "def create_substrings(trial, guessed=[], n=6, threshold=0.1, multiple=False, words=[]):\n",
        "    \"\"\"\n",
        "    Creates substrings around missing letters to apply heuristic analysis.\n",
        "\n",
        "    Parameters:\n",
        "        trial (str): Current word state with underscores\n",
        "        guessed (list): List of already guessed letters\n",
        "        n (int): Substring length\n",
        "        threshold (float): Confidence threshold\n",
        "        multiple (bool): Whether to consider multiple missing letters\n",
        "        words (list): List of all training words\n",
        "\n",
        "    Returns:\n",
        "        tuple: (most probable letter, probability) or (None, None)\n",
        "    \"\"\"\n",
        "    substring = []\n",
        "    for i in range(len(trial)):\n",
        "        if trial[i] == '_':\n",
        "            if multiple:\n",
        "                # Replace all '_' with '.' for regex flexibility\n",
        "                substring.append((trial[max(i-n+1,0):i] + '.' + trial[i+1:min(i+n,len(trial))]).replace(\"_\",\"*\"))\n",
        "            else:\n",
        "                # Only consider single missing letters\n",
        "                if (i == 0 or trial[i-1] != '_') and (i == len(trial)-1 or trial[i+1] != '_'):\n",
        "                    substring.append(trial[max(i-n+1,0):i] + '.' + trial[i+1:min(i+n,len(trial))])\n",
        "\n",
        "    # Extract substrings of exact length n\n",
        "    substrings = []\n",
        "    for s in substring:\n",
        "        for j in range(len(s)-n+1):\n",
        "            substrings.append(s[j:j+n])\n",
        "\n",
        "    # Filter substrings based on missing letter count\n",
        "    if multiple:\n",
        "        substrings = [s for s in substrings if s.count('*') < 2]\n",
        "    else:\n",
        "        substrings = [s for s in substrings if s.count('_') == 0]\n",
        "\n",
        "    substrings = [s for s in substrings if len(s) == n]\n",
        "\n",
        "    if not substrings:\n",
        "        return None, None\n",
        "\n",
        "    ans = []\n",
        "    for s in substrings:\n",
        "        ind = s.index('.') if '.' in s else s.index('*')  # Find the position of missing letter\n",
        "        letters = []\n",
        "        for word in words:\n",
        "            match = re.search(s.replace('*', '.'), word)\n",
        "            if match:\n",
        "                l = word[match.start()+ind]\n",
        "                if l not in guessed:\n",
        "                    letters.append(l)\n",
        "        if not letters:\n",
        "            continue\n",
        "        # Calculate frequency\n",
        "        counter = collections.Counter(letters)\n",
        "        total = len(letters)\n",
        "        freq = [[char, count / total, count] for char, count in counter.most_common()]\n",
        "        ans.append(freq)\n",
        "\n",
        "    if not ans:\n",
        "        return None, None\n",
        "\n",
        "    # Aggregate frequencies\n",
        "    bb = np.zeros(26)\n",
        "    for a in ans:\n",
        "        for entry in a:\n",
        "            ch, prob, _ = entry\n",
        "            bb[value_map[ch]-1] += prob\n",
        "\n",
        "    if bb.sum() > 0:\n",
        "        bb /= bb.sum()  # Normalize probabilities\n",
        "    else:\n",
        "        bb = np.zeros(26)\n",
        "\n",
        "    if bb.max() < threshold:\n",
        "        return None, None\n",
        "\n",
        "    return value_rev[bb.argmax()+1], bb.max()\n",
        "\n",
        "\n",
        "def calculate_known_percentage(word_segment):\n",
        "    \"\"\"\n",
        "    Calculate the percentage of known letters in a word segment.\n",
        "\n",
        "    :param word_segment: string of the word segment to analyze\n",
        "    :return: float percentage of known letters\n",
        "    \"\"\"\n",
        "    if not word_segment:\n",
        "        return 0.0\n",
        "\n",
        "    total_letters = len(word_segment)\n",
        "    known_letters = sum(1 for char in word_segment if char != '_')\n",
        "    return (known_letters / total_letters) * 100\n",
        "\n",
        "def guess_next_letter_suff_pref(trial_word, guessed_letters=None):\n",
        "    \"\"\"\n",
        "    Guess the next letter based on common prefix and suffix heuristics.\n",
        "\n",
        "    :param trial_word: string representing the current state of the word, e.g., \"a _ t _ m a _ i c\"\n",
        "    :param common_prefixes: list of common prefixes\n",
        "    :param common_suffixes: list of common suffixes\n",
        "    :param guessed_letters: set of letters already guessed (optional)\n",
        "    :return: the next letter to guess based on heuristics, or None if no guess can be made\n",
        "    \"\"\"\n",
        "    # Define common prefixes and suffixes\n",
        "    common_prefixes = [\n",
        "        \"aero\", \"agri\", \"allo\", \"ambi\", \"amphi\",\n",
        "        \"ante\", \"anti\", \"arch\", \"astro\", \"audio\",\n",
        "        \"auto\", \"bene\", \"biblio\", \"cardio\", \"chrono\",\n",
        "        \"circum\", \"contra\", \"counter\", \"crypto\", \"cryo\",\n",
        "        \"cyber\", \"demi\", \"demo\", \"derma\", \"ecto\",\n",
        "        \"electro\", \"endo\", \"ethno\", \"extra\", \"fore\",\n",
        "        \"gastro\", \"gyro\", \"helio\", \"hemi\", \"hetero\",\n",
        "        \"hexo\", \"homo\", \"hyper\", \"hydro\", \"hypo\",\n",
        "        \"ideo\", \"indo\", \"infra\", \"inter\", \"intra\",\n",
        "        \"iso\", \"litho\", \"macro\", \"mammo\", \"mega\",\n",
        "        \"meso\", \"meta\", \"micro\", \"mini\", \"mono\",\n",
        "        \"multi\", \"necro\", \"neuro\", \"omni\", \"over\",\n",
        "        \"para\", \"peri\", \"phono\", \"photo\", \"poly\",\n",
        "        \"post\", \"proto\", \"pseudo\", \"psycho\", \"pyro\",\n",
        "        \"quad\", \"retro\", \"rhino\", \"semi\", \"socio\",\n",
        "        \"stereo\", \"super\", \"supra\", \"techno\", \"tele\",\n",
        "        \"theo\", \"thermo\", \"trans\", \"typo\", \"ultra\",\n",
        "        \"under\", \"vice\", \"xeno\", \"peri\"\n",
        "    ]\n",
        "\n",
        "    common_suffixes = [\n",
        "        \"able\", \"aceous\", \"algia\", \"ance\", \"archy\",\n",
        "        \"arian\", \"arium\", \"atic\", \"ation\", \"ative\",\n",
        "        \"cidal\", \"cide\", \"ciate\", \"cracy\", \"dial\",\"cally\"\n",
        "        \"ectomy\", \"ence\", \"eous\", \"escent\", \"esque\",\n",
        "        \"etic\", \"fold\", \"fully\", \"graph\", \"hood\",\n",
        "        \"ible\", \"icide\", \"icity\", \"iform\", \"ious\",\n",
        "        \"itis\", \"itude\", \"ness\", \"lling\", \"logy\",\n",
        "        \"mania\", \"ment\", \"meter\", \"morph\", \"most\",\n",
        "        \"less\", \"ology\", \"osis\", \"phile\", \"phobe\",\n",
        "        \"scope\", \"scopy\", \"ship\", \"sion\", \"some\",\n",
        "        \"sophy\", \"ster\", \"therm\", \"tious\", \"tomy\",\n",
        "        \"tude\", \"ulate\", \"ulent\", \"ville\", \"ward\",\n",
        "        \"wards\", \"wise\", \"worthy\"\n",
        "    ]\n",
        "\n",
        "    # Remove spaces and convert to lowercase for uniformity\n",
        "    word = trial_word.replace(' ', '').lower()\n",
        "\n",
        "    if guessed_letters is None:\n",
        "        guessed_letters = set()\n",
        "\n",
        "    # Prefix Heuristic\n",
        "    for prefix in common_prefixes:\n",
        "        if len(prefix) > len(word):\n",
        "            continue\n",
        "\n",
        "        prefix_match = True\n",
        "        missing_letters = []\n",
        "        word_segment = word[:len(prefix)]\n",
        "\n",
        "        # Calculate known percentage for the word segment that would match the prefix\n",
        "        known_percentage = calculate_known_percentage(word_segment)\n",
        "\n",
        "        # Only proceed if known percentage is > 60%\n",
        "        if known_percentage <= 55:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(prefix)):\n",
        "            word_char = word[i]\n",
        "            prefix_char = prefix[i]\n",
        "\n",
        "            if word_char == '_':\n",
        "                if prefix_char not in guessed_letters:\n",
        "                    missing_letters.append(prefix_char)\n",
        "            elif word_char != prefix_char:\n",
        "                prefix_match = False\n",
        "                break\n",
        "\n",
        "        if prefix_match and missing_letters:\n",
        "            return missing_letters[0]\n",
        "\n",
        "    # Suffix Heuristic\n",
        "    for suffix in common_suffixes:\n",
        "        if len(suffix) > len(word):\n",
        "            continue\n",
        "\n",
        "        suffix_match = True\n",
        "        missing_letters = []\n",
        "        word_segment = word[-len(suffix):]\n",
        "\n",
        "        # Calculate known percentage for the word segment that would match the suffix\n",
        "        known_percentage = calculate_known_percentage(word_segment)\n",
        "\n",
        "        # Only proceed if known percentage is > 60%\n",
        "        if known_percentage <= 65:\n",
        "            continue\n",
        "\n",
        "        for i in range(1, len(suffix) + 1):\n",
        "            word_char = word[-i]\n",
        "            suffix_char = suffix[-i]\n",
        "\n",
        "            if word_char == '_':\n",
        "                if suffix_char not in guessed_letters:\n",
        "                    missing_letters.append(suffix_char)\n",
        "            elif word_char != suffix_char:\n",
        "                suffix_match = False\n",
        "                break\n",
        "\n",
        "        if suffix_match and missing_letters:\n",
        "            return missing_letters[0]\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj4cpi4LerL3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Updated HangmanAPI Class with Ensemble Integration\n",
        "class HangmanAPI(object):\n",
        "    def __init__(self, access_token=None, session=None, timeout=None, ensemble_model_path='ensemble_model_xgb_catboost.pkl', catboost_model_path='multilabel_catboost_model.pkl', xgboost_model_path='multilabel_xgb_model.pkl', meta_classifier_path='meta_classifiers.pkl'):\n",
        "        self.hangman_url = self.determine_hangman_url()\n",
        "        self.access_token = access_token\n",
        "        self.session = session or requests.Session()\n",
        "        self.timeout = timeout\n",
        "        self.guessed_letters = []\n",
        "        self.words = self.build_dictionary(\"words_250000_train.txt\")\n",
        "        self.successforgame=0\n",
        "        # Initialize the ensemble model\n",
        "        self.ensemble_model = MultiLabelEnsemble(catboost_model_path, xgboost_model_path, meta_classifier_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def determine_hangman_url():\n",
        "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
        "        data = {link: 0 for link in links}\n",
        "        for link in links:\n",
        "            for _ in range(10):\n",
        "                s = time.time()\n",
        "                try:\n",
        "                    requests.get(link, timeout=1)\n",
        "                    data[link] += time.time() - s\n",
        "                except:\n",
        "                    data[link] += float('inf')\n",
        "        # Select the fastest link\n",
        "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
        "        link += '/trexsim/hangman'\n",
        "        return link\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def guess(self, word):\n",
        "        \"\"\"\n",
        "        Enhanced guess function using ensemble model and heuristics.\n",
        "\n",
        "        Parameters:\n",
        "            word (str): Current word state with spaces (e.g., \"_ p p _ e\")\n",
        "\n",
        "        Returns:\n",
        "            str: Next letter to guess\n",
        "        \"\"\"\n",
        "        # Clean the word: remove spaces\n",
        "        trial_word = word.replace(\" \", \"\")\n",
        "\n",
        "        # Encode the current word state\n",
        "        encoded = encode_word(trial_word).reshape(1, -1)  # Shape: (1, 80)\n",
        "\n",
        "        # Convert to DataFrame if necessary\n",
        "        if isinstance(encoded, np.ndarray):\n",
        "            X = pd.DataFrame(encoded, columns=[str(i) for i in range(80)])\n",
        "        else:\n",
        "            X = encoded\n",
        "\n",
        "        # Get ensemble predictions\n",
        "        probs = self.ensemble_model.predict_proba(X)[0]  # Shape: (26,)\n",
        "\n",
        "        # Create a probability dictionary\n",
        "        prob_dict = {ch: probs[i] for i, ch in enumerate(alphabet)}\n",
        "\n",
        "        # Sort letters by probability in descending order\n",
        "        sorted_probs = sorted(prob_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Heuristics: Substring, Prefix, Suffix, Vowels\n",
        "        # Prioritize using heuristics to adjust probabilities\n",
        "\n",
        "        # Apply Substring Heuristics\n",
        "        letter_from_substr, substr_prob = create_substrings(\n",
        "            trial_word,\n",
        "            guessed=self.guessed_letters,\n",
        "            n=6,\n",
        "            threshold=0.1,\n",
        "            multiple=True,\n",
        "            words=self.words\n",
        "        )\n",
        "\n",
        "        if letter_from_substr and substr_prob > 0.2:\n",
        "            if letter_from_substr not in self.guessed_letters:\n",
        "                return letter_from_substr\n",
        "\n",
        "        # Apply Prefix/Suffix Heuristics (common prefixes/suffixes)\n",
        "        # Define common prefixes and suffixes\n",
        "        letter_from_pref_suffix = guess_next_letter_suff_pref(trial_word=word, guessed_letters=self.guessed_letters)\n",
        "        if(letter_from_pref_suffix!=None):\n",
        "            print(\"Letter from prefix and suffix\")\n",
        "            return letter_from_pref_suffix\n",
        "\n",
        "        # Prioritize vowels if not enough information\n",
        "        vowels = ['a', 'e', 'i', 'o', 'u']\n",
        "        # Sort vowels by their probability and pick the highest one not guessed yet\n",
        "        vowels_sorted = sorted([v for v in vowels if v not in self.guessed_letters], key=lambda x: -prob_dict.get(x, 0))\n",
        "        max_vowels_guess=2\n",
        "        if(len(word)>5):\n",
        "            max_vowels_guess=3\n",
        "        if vowels_sorted and len(self.guessed_letters)<max_vowels_guess:\n",
        "            return vowels_sorted[0]\n",
        "\n",
        "        # If no heuristics apply, pick the highest probability letter not guessed yet\n",
        "        for ch, prob in sorted_probs:\n",
        "            if ch not in self.guessed_letters:\n",
        "                return ch\n",
        "\n",
        "        # Fallback: pick a random letter not guessed yet\n",
        "        remaining_letters = [ch for ch in alphabet if ch not in self.guessed_letters]\n",
        "        if remaining_letters:\n",
        "            return random.choice(remaining_letters)\n",
        "\n",
        "        # If all letters guessed, return a random letter (shouldn't happen)\n",
        "        return random.choice(alphabet)\n",
        "\n",
        "    def build_dictionary(self, dictionary_file_location):\n",
        "        \"\"\"Read dictionary words from a text file.\"\"\"\n",
        "        with open(dictionary_file_location, \"r\") as f:\n",
        "            full_dictionary = f.read().splitlines()\n",
        "        return full_dictionary\n",
        "\n",
        "    def start_game(self, practice=True, verbose=True):\n",
        "        # Reset guessed letters\n",
        "        self.guessed_letters = []\n",
        "\n",
        "        response = self.request(\"/new_game\", {\"practice\": practice})\n",
        "        if response.get('status') == \"approved\":\n",
        "            game_id = response.get('game_id')\n",
        "            word = response.get('word')  # Initial masked word\n",
        "            tries_remains = response.get('tries_remains')\n",
        "            if verbose:\n",
        "                print(f\"New game started! Game ID: {game_id}. Tries remaining: {tries_remains}. Word: {word}\")\n",
        "            while tries_remains > 0:\n",
        "                # Get guessed letter from the enhanced guess function\n",
        "                guess_letter = self.guess(word)\n",
        "\n",
        "                # Append guessed letter to guessed letters\n",
        "                self.guessed_letters.append(guess_letter)\n",
        "                if verbose:\n",
        "                    print(f\"Guessing letter: {guess_letter}\")\n",
        "\n",
        "                try:\n",
        "                    res = self.request(\"/guess_letter\", {\"request\": \"guess_letter\", \"game_id\": game_id, \"letter\": guess_letter})\n",
        "                except HangmanAPIError:\n",
        "                    print('HangmanAPIError exception caught on request.')\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    print('Other exception caught on request.')\n",
        "                    raise e\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"Server response: {res}\")\n",
        "\n",
        "                status = res.get('status')\n",
        "                tries_remains = res.get('tries_remains')\n",
        "                if status == \"success\":\n",
        "                    self.successforgame+=1\n",
        "                    if verbose:\n",
        "                        print(f\"Successfully guessed the word: {res.get('word')}\")\n",
        "                    return True\n",
        "                elif status == \"failed\":\n",
        "                    reason = res.get('reason', '# of tries exceeded!')\n",
        "                    if verbose:\n",
        "                        print(f\"Failed to guess the word. Reason: {reason}\")\n",
        "                    return False\n",
        "                elif status == \"ongoing\":\n",
        "                    word = res.get('word')  # Update the masked word\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"Failed to start a new game\")\n",
        "        return status == \"success\"\n",
        "\n",
        "    ##########################################################\n",
        "    # The following methods remain unchanged from your original code\n",
        "    ##########################################################\n",
        "\n",
        "    def my_status(self):\n",
        "        return self.request(\"/my_status\", {})\n",
        "\n",
        "    def request(self, path, args=None, post_args=None, method=None):\n",
        "        if args is None:\n",
        "            args = dict()\n",
        "        if post_args is not None:\n",
        "            method = \"POST\"\n",
        "\n",
        "        # Add `access_token` to post_args or args if it has not already been included.\n",
        "        if self.access_token:\n",
        "            if post_args and \"access_token\" not in post_args:\n",
        "                post_args[\"access_token\"] = self.access_token\n",
        "            elif \"access_token\" not in args:\n",
        "                args[\"access_token\"] = self.access_token\n",
        "\n",
        "        time.sleep(0.2)\n",
        "\n",
        "        num_retry, time_sleep = 50, 2\n",
        "        for it in range(num_retry):\n",
        "            try:\n",
        "                response = self.session.request(\n",
        "                    method or \"GET\",\n",
        "                    self.hangman_url + path,\n",
        "                    timeout=self.timeout,\n",
        "                    params=args,\n",
        "                    data=post_args,\n",
        "                    verify=False\n",
        "                )\n",
        "                break\n",
        "            except requests.HTTPError as e:\n",
        "                response = json.loads(e.read())\n",
        "                raise HangmanAPIError(response)\n",
        "            except requests.exceptions.SSLError as e:\n",
        "                if it + 1 == num_retry:\n",
        "                    raise\n",
        "                time.sleep(time_sleep)\n",
        "\n",
        "        headers = response.headers\n",
        "        if 'json' in headers.get('content-type', ''):\n",
        "            result = response.json()\n",
        "        elif \"access_token\" in parse_qs(response.text):\n",
        "            query_str = parse_qs(response.text)\n",
        "            if \"access_token\" in query_str:\n",
        "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
        "                if \"expires\" in query_str:\n",
        "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
        "            else:\n",
        "                raise HangmanAPIError(response.json())\n",
        "        else:\n",
        "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
        "\n",
        "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
        "            raise HangmanAPIError(result)\n",
        "        return result\n",
        "\n",
        "class HangmanAPIError(Exception):\n",
        "    def __init__(self, result):\n",
        "        self.result = result\n",
        "        self.code = None\n",
        "        try:\n",
        "            self.type = result[\"error_code\"]\n",
        "        except (KeyError, TypeError):\n",
        "            self.type = \"\"\n",
        "\n",
        "        try:\n",
        "            self.message = result[\"error_description\"]\n",
        "        except (KeyError, TypeError):\n",
        "            try:\n",
        "                self.message = result[\"error\"][\"message\"]\n",
        "                self.code = result[\"error\"].get(\"code\")\n",
        "                if not self.type:\n",
        "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
        "            except (KeyError, TypeError):\n",
        "                try:\n",
        "                    self.message = result[\"error_msg\"]\n",
        "                except (KeyError, TypeError):\n",
        "                    self.message = result\n",
        "        Exception.__init__(self, self.message)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YLaHieoerL6"
      },
      "outputs": [],
      "source": [
        "class MultiLabelXGBoostClassifier:\n",
        "    \"\"\"\n",
        "    Trains 26 separate XGBoost binary classifiers, each predicting whether\n",
        "    a letter is needed in the guess set.\n",
        "    Includes checkpointing to save progress after each classifier is trained.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = 26, xgb_params: Optional[Dict] = None):\n",
        "        self.num_classes = num_classes\n",
        "        # Default XGBoost params can be overridden\n",
        "        default_params = {\n",
        "            \"n_estimators\": 100,\n",
        "            \"max_depth\": 6,\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"use_label_encoder\": False,\n",
        "            \"eval_metric\": \"logloss\",\n",
        "            \"verbosity\": 0  # Suppress XGBoost's own logs\n",
        "        }\n",
        "        if xgb_params is not None:\n",
        "            default_params.update(xgb_params)\n",
        "        self.xgb_params = default_params\n",
        "\n",
        "        # Create a list of XGBClassifier models\n",
        "        self.models: List[xgb.XGBClassifier] = [\n",
        "            xgb.XGBClassifier(**self.xgb_params) for _ in range(num_classes)\n",
        "        ]\n",
        "\n",
        "        # Keep track of trained classifier indices\n",
        "        self.trained_indices = set()\n",
        "\n",
        "    def train_single_classifier(self, i: int, X: pd.DataFrame, y_i: np.ndarray):\n",
        "        \"\"\"\n",
        "        Trains a single classifier for the given index.\n",
        "        \"\"\"\n",
        "        letter = alphabet[i]\n",
        "        print(f\"Training classifier for letter '{letter}'...\")\n",
        "        self.models[i].fit(X, y_i)\n",
        "        self.trained_indices.add(i)\n",
        "        print(f\"Classifier for letter '{letter}' trained.\")\n",
        "\n",
        "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns an array of shape (n_samples, 26) with the predicted probability\n",
        "        for each letter being '1' (i.e., needed).\n",
        "        \"\"\"\n",
        "        all_preds = []\n",
        "        print(\"Generating probability predictions...\")\n",
        "        # Initialize tqdm progress bar\n",
        "        # progress_bar = tqdm(range(self.num_classes), desc=\"Predicting Probabilities\", unit=\"classifier\", dynamic_ncols=True)\n",
        "        for i in range(self.num_classes):\n",
        "            letter = alphabet[i]\n",
        "            preds_i = self.models[i].predict_proba(X)[:, 1]\n",
        "            all_preds.append(preds_i)\n",
        "        return np.array(all_preds).T  # shape => (n_samples, 26)\n",
        "\n",
        "    def save(self, filename: str):\n",
        "        \"\"\"\n",
        "        Saves the trained model to a file.\n",
        "        \"\"\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "        print(f\"Model saved to {filename}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename: str):\n",
        "        \"\"\"\n",
        "        Loads a trained model from a file.\n",
        "        \"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        print(f\"Model loaded from {filename}\")\n",
        "        return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv7WBopYerL8"
      },
      "outputs": [],
      "source": [
        "class MultiLabelCatBoostClassifier:\n",
        "    \"\"\"\n",
        "    Trains 26 CatBoost binary classifiers (one per letter).\n",
        "    Each classifier predicts if a letter is 'hidden' (i.e., in the word but not revealed in the partial encoding).\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=26, catboost_params=None):\n",
        "        if catboost_params is None:\n",
        "            catboost_params = {\n",
        "                \"iterations\": 1500,\n",
        "                \"task_type\": \"GPU\",\n",
        "                \"verbose\": False\n",
        "            }\n",
        "        # Create 26 CatBoost classifiers\n",
        "        self.classifiers = [\n",
        "            CatBoostClassifier(**catboost_params) for _ in range(num_classes)\n",
        "        ]\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for i in range(self.num_classes):\n",
        "            self.classifiers[i].fit(X, y[alpha[i]], verbose=100)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Return predicted probability for each of the 26 letters.\n",
        "        Shape: (n_samples, 26)\n",
        "        \"\"\"\n",
        "        num_classes=26\n",
        "        predictions = np.zeros((len(X), num_classes))\n",
        "        for i, clf in enumerate(self.classifiers):\n",
        "            # Probability that letter i is actually hidden in the word\n",
        "            predictions[:, i] = clf.predict_proba(X)[:, 1]\n",
        "        return predictions\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEscYFQserL9"
      },
      "outputs": [],
      "source": [
        "api = HangmanAPI(\n",
        "        access_token=\"82ed141920582322b3c4a774b98cbd\",\n",
        "        timeout=2000,\n",
        "        ensemble_model_path=\"ensemble_model_xgb_catboost.pkl\",\n",
        "        catboost_model_path=\"multilabel_catboost_model.pkl\",\n",
        "        xgboost_model_path=\"multilabel_xgb_model.pkl\",\n",
        "        meta_classifier_path=\"meta_classifiers.pkl\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EF9-jqSerL_",
        "outputId": "bdc2928f-8567-41cf-b59a-4048becd54fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.6.0\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7jZZw-herMC",
        "outputId": "e42d3360-df14-4bb2-c890-d8fa5428d9a7"
      },
      "outputs": [
        {
          "ename": "HangmanAPIError",
          "evalue": "{'error': 'Your account has been deactivated!'}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHangmanAPIError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(runs \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m      5\u001b[0m     runs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpractice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     [total_practice_runs, total_recorded_runs, total_recorded_successes, total_practice_successes] \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mmy_status() \u001b[38;5;66;03m# Get my game stats: (# of tries, # of wins)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     practice_success_rate \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39msuccessforgame \u001b[38;5;241m/\u001b[39m runs\n",
            "Cell \u001b[1;32mIn[3], line 122\u001b[0m, in \u001b[0;36mHangmanAPI.start_game\u001b[1;34m(self, practice, verbose)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_game\u001b[39m(\u001b[38;5;28mself\u001b[39m, practice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# Reset guessed letters\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguessed_letters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 122\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/new_game\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpractice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpractice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproved\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    124\u001b[0m         game_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[3], line 226\u001b[0m, in \u001b[0;36mHangmanAPI.request\u001b[1;34m(self, path, args, post_args, method)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaintype was not text, or querystring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(result)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[1;31mHangmanAPIError\u001b[0m: {'error': 'Your account has been deactivated!'}"
          ]
        }
      ],
      "source": [
        "api.successforgame=0\n",
        "\n",
        "runs = 0\n",
        "while(runs < 50):\n",
        "    runs += 1\n",
        "    api.start_game(practice=1, verbose=True)\n",
        "    [total_practice_runs, total_recorded_runs, total_recorded_successes, total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
        "    practice_success_rate = api.successforgame / runs\n",
        "    print('run %d game. practice success rate so far = %.3f' % (runs, practice_success_rate))\n",
        "    # time.sleep(1.0)\n",
        "    print(time.localtime())\n",
        "\n",
        "print(\"DONE\")\n",
        "practice_success_rate = api.successforgame / runs\n",
        "print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN5hYWuOerME",
        "outputId": "a63f7637-bf40-4153-c1e5-abed7f121570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Playing  0  th game\n"
          ]
        },
        {
          "ename": "HangmanAPIError",
          "evalue": "{'error': 'Your account has been deactivated!'}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHangmanAPIError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m    \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpractice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     api\u001b[38;5;241m.\u001b[39mstart_game(practice\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "Cell \u001b[1;32mIn[3], line 122\u001b[0m, in \u001b[0;36mHangmanAPI.start_game\u001b[1;34m(self, practice, verbose)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_game\u001b[39m(\u001b[38;5;28mself\u001b[39m, practice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# Reset guessed letters\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguessed_letters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 122\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/new_game\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpractice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpractice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproved\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    124\u001b[0m         game_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[3], line 226\u001b[0m, in \u001b[0;36mHangmanAPI.request\u001b[1;34m(self, path, args, post_args, method)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaintype was not text, or querystring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(result)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[1;31mHangmanAPIError\u001b[0m: {'error': 'Your account has been deactivated!'}"
          ]
        }
      ],
      "source": [
        "for i in range(1000):\n",
        "    print('Playing ', i, ' th game')\n",
        "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
        "    if(i<10):\n",
        "       api.start_game(practice=0,verbose=True)\n",
        "    else:\n",
        "        api.start_game(practice=0,verbose=False)\n",
        "\n",
        "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
        "    time.sleep(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYIPhMUherMG",
        "outputId": "68f40f44-0544-4bcf-f6d1-2b6e00a8f3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "overall success rate = 0.597\n"
          ]
        }
      ],
      "source": [
        "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
        "success_rate = total_recorded_successes/total_recorded_runs\n",
        "print('overall success rate = %.3f' % success_rate)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}